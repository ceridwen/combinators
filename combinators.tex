\documentclass[12pt]{article}

\title{Modern Parser Combinators in Python}
\date{\today}

\usepackage[sc,osf]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{listings}
% \lstset{language=Python}
\lstset{escapechar=\!}

\usepackage[backend=bibtex8,style=authoryear]{biblatex}
\bibliography{combinators}


\begin{document}

\maketitle

\tableofcontents

\section{Overview}
\label{sec:overview}

The big idea behind this library is to combine techniques from the
parsing literature, some new, some old and neglected, to create a
library that can compete with hand-written recursive-descent parsers.
The key advantages that hand-written recursive-descent parsers have
over traditional LALR parser generators are simplicity, handling a
broader range of grammars, better error reporting, and easier
introduction of custom code.  A well-designed library can have most of
these, too, and some advantages over a recursive-descent parser, at
the cost of an implementation that isn't as simple.

\begin{itemize}
\item Parser combinators make parsers easier to learn and write by
  embedding a parsing library into a general-purpose programming
  language, making it easy to add custom code to the parser and not
  requiring that anyone learn a DSL like EBNF or formal grammars.
\item GLL or Earley can parse any CFG, including ambiguous and
  left-recursive grammars, in $O(n^3)$ time.  % , and
  % unambiguous grammars in linear time, all while remaining top-down
  % depth-first like recursive descent and thus easier to understand and
  % debug.
\item Attribute grammars can allow the insertion of arbitrary code
  into the parsing process, principally allowing the parsing of
  context-sensitive grammars that a general CFG algorithm can't handle
  alone, and with the data-dependent grammar restrictions, parse them
  in $O(n^3)$ time.  They also provide an easy mechanism for adding
  semantic actions.
\item Non-correcting error handling can locate and provide reasonable
  messages for possible errors in an input string without
  providing any spurious messages.
\item Metaprogramming techniques like macros or staging can make
  parser combinators almost as efficient as recursive descent.
\item Attribute grammars can be inverted to allow writing both a
  parser and an unparser at the same time.
\end{itemize}


\section{Design Decisions}
\label{sec:design_decisions}


\subsection{Grammars}
\label{sec:grammar}

My goal for these parser combinators is that they should be able to
handle common languages and formats \emph{without} having to write
custom code except for extremely weird cases.  LL and LR grammars are
too limited: many natural grammars for common problems are not in
LL($k$) or LR($k$) for any $k$, and LR parsers, which can handle more
grammars for any given value of $k$, are difficult to debug.  For
instance, the natural way to write a grammar for a left-associative
binary operation uses left recursion.  Algorithms that can parse any
CFG are easier to write grammars for and produce better parse trees.
CFLs also have better closure properties than LL or LR languages.
They're closed under language composition, that is, they're closed
under union, concatenation, and Kleene star.  Parsing problems with
one language embedded in another are becoming increasingly common, and
the ability to write parsers for two different languages and then
combine them is useful.  Also, the suffix language of a CFL is another
CFL \parencite[p. 401]{grune_jacobs} and so is its reverse language,
both of which are useful for producing better error messages
(\ref{sec:errors}).

Unfortunately, CFGs are also not powerful enough to handle common
parsing problems.  Real-world programming languages and data formats
are context sensitive: C has the \texttt{typedef-name: identifier}
problem, Python has context-sensitive indentation, real-world HTML has
context-sensitive features, such as where \texttt{<input>} tags can
appear, and \TeX's DVI files have references to byte offsets in the
file.  Thus, a good parsing library needs to be able to handle context
sensitivity.  Note that one example often presented as context
sensitive, a field prefixed with a byte giving the field length like a
Pascal string, is actually regular: because a byte can only hold a
finite set of values, it's possible to write a regular expression for
the whole construct like,

\begin{lstlisting}
0 | 1. | 2.. | !\ldots! | 255.{255}
\end{lstlisting}

where \texttt{.} represents any character and a number in curly braces
represents repetition as usual.

This technique generalizes to any field prefixed with a finite-size
integer and beyond.  In any language with finite symbols, any
length-type-value array (also type-length-value;
\url{https://en.wikipedia.org/wiki/Type-length-value}) can produce
only a finite language because the set of possible lengths and
possible types are limited.  When the production rule for a
length-type-value array is inserted into a grammar producing an
infinite language, the length-type-value array effectively acts as a
terminal because it can only create a finite set of productions, thus
it can't introduce context-sensitivity unless it's explicitly
introduced elsewhere.  If all the integers are single symbols for the
lengths, \texttt{a}, \texttt{b}, \texttt{c}, \ldots are types, and
\texttt{A}, \texttt{B}, \texttt{C}, \ldots are the production rules
corresponding to each type, these can be expressed using the
production rule:

\begin{lstlisting}
S !$\to$! 0a | 0b | 0c | !\ldots! | 1aA | 1bB | 1cC | !\ldots! |
2aAA | 2bBB | 2cCC | !\ldots!
\end{lstlisting}

This kind of use of large but finite productions can handle even some
weird cases like lengths of code that are then treated like their own
languages.  For instance, if nonterminal \texttt{A} has length 1
always and \texttt{B} length 2,

\begin{lstlisting}
S !$\to$! 0 | 1A | 2AA | 2B | 3AAA | 3AB | 3BA | !\ldots!
\end{lstlisting}

However, \emph{efficiently} handling these finite production rules is
another can of worms, because they can be exponential in the number of
symbols in the language and thus any DFA for them would be
impractical.

I chose as my context-sensitive grammar the data-dependent grammars of
\textcite{yakker1}, which are an extension of CFGs related to
L-attribute grammars.  DDGs have several attractive properties.
First, they're easier to understand and write than the primary
alternatives, PEGs and Boolean grammars, because they use familiar
concepts like variable binding and string recognition based on
semantic values, for instance treating bytes as integers rather than
abstract symbols.  While it's often easy enough to write a PEG or a
Boolean grammar for simple languages like $a^nb^nc^n$, something like
a field prefixed with a length in decimal as ASCII digits requires
multiple rules like the above for length-type-value arrays.
Meanwhile, a DDG simply calls a Turing-computable function to
transform the ASCII into an integer.  This touches on DDGs' second
major advantage, which is the ability to include arbitrary
Turing-computable code in a disciplined fashion.

\begin{quote}
  This lack of enthousiasm in incorporating attribute grammars into
  formal language and automata theory may be due partly to the
  complexity of the model and partly to the (obvious) fact that
  attribute grammars of a very simple type, having only one
  synthesized attribute and no inherited attributes, can already
  realize all possible translations [18] and can e.g. easily simulate
  type 0 Chomsky grammars (cf. [19]). \parencite{1V_AGs}
\end{quote}

The power of DDGs makes analyzing them harder but means that they can
parse anything and can incorporate existing specialized parser code
when necessary.  Third, they're based on CFG parsing algorithms, which
makes them easier to adapt for one of the existing efficient CFG
algorithms, with the original authors implementing them in particular
for combinators, for Earley, and for GLR \parencite{yakker2}.  Fourth,
their languages can be parsed in $O(n^3)$ time in the worst case, or
only $O(n)$ for deterministic languages and $O(n^2)$ for
locally-ambiguous languages.  This is a consequence of the single-pass
nature of DDGs: similar to L-attribute grammars, information flows up
and from left to right in the parse tree with no backtracking allowed.
Fifth, the parsing algorithm for DDGs is proved correct assuming the
underlying CFG parsing algorithm is correct.  Sixth, DDGs inherit from
CFGs important closure properties, also being closed under union,
concatenation, and Kleene star. A similar construction to the one for
CFGs shows that the suffix language of a DDG can be described by
another DDG, with one wrinkle: the parser always has to take all
options, treating them as alternatives, when encountering a constraint
based on a variable that has yet to be defined.

TODO: work out the reverse language for a DDG and figure out
something to say about it here.  Also work out a formal notion of
undefined for attributes: if a constraint tries to access an
undefined attribute, it falls back to parsing all alteratives.  If a
attribute function receives an undefined attribute as an input, it
always returns undefined.

The two major alternatives I considered were PEGs and Boolean
grammars.  They both have the disadvantage that to parse arbitrary
Turing-computable properties requires further extending them.  The
major advantage of PEGs is that the corresponding parsing algorithm,
the packrat parser, always runs in $O(n)$ time.  However, modern CFG
algorithms can parse most CFGs in linear time, and the ones that they
can't are highly ambiguous and only found in a subset of real parsing
problems like parsing DNA.  My experiments indicate that it's possible
to parse many CFGs in $O(n)$ time without any memoization, and
experiments in \textcite{packrat_is_it_worth_it} back this up, showing
that no memoization is often close to optimal.  With the high
preexisting memory cost of the parse tree, the memory costs of the
packrat parser make running out of memory even more likely.  Aside
from the memory issues, PEGs have three other disadvantages.  Making a
PEG parser handle left recursion makes the implementation much more
complicated, particularly when dealing with indirect left recursion,
and costs the linear-time guarantee.  PEGs can't be composed in the
same way that CFGs can, since PEGs ``are closed under union, but only
under a non-standard definition of what it means for a string to be in
a language (the PEG need only succeed on a prefix of the
string)'' \parencite[p. 4]{yakker1}.  This and other problems are due
to the non-commutativity of alternation in PEGs.  Similarly, the way
PEGs avoid ambiguity makes it hard to explicitly deal with it where it
naturally arises, for instance in unparsing the AST for a language
that ignores white space or when trying to do error handling using the
suffix language.

Boolean grammars are a promising approach because they share
properties like closure under union, concatenation, Kleene star,
suffix, and reversal.  However, there hasn't been enough work yet done
on developing practical parsers for them.  Many of the algorithms
known to work for CFGs have not been extended to Boolean grammars.  In
particular, neither of the algorithms most friendly for parser
combinators, GLL nor Earley, have been, if they even can be extended.
There's also nothing on other aspects of building a practical parser
like error handling.


\subsection{Algorithms}
\label{sec:algorithms}

% The backend parsing algorithm will be GLL modified to include the
% DDG/L-AG features.

There are about four different algorithms for parsing arbitrary CFGs,
GLL, GLR, Earley, and CYK/Valiant's algorithm.  While Valiant's has
the best worst-case time complexity because it depends on matrix
multiplication and there are matrix multiplication algorithms with
worst-case run time better than $O(n^3)$, those algorithms require
more memory than the naive matrix multiplication algorithm and have
worse constant factors (Coppersmith-Winograd has terrible constant
factors it's only faster on matrices larger than current hardware can
accommodate), and CYK/Valiant's algorithm also has worse performance on
deterministic grammars than the others.  Thus, it's a theoretical
curiosity, not useful for practical parsing.  The other algorithms all
have worst-case complexity of $O(n^3)$.  Since the worst-case
complexity is the same, the real performance considerations are
asymptotic behavior on typical, i.e. not highly ambiguous, grammars
and constant factors.

Earley's algorithm with Leo's modifications can parse all LR-regular
languages in linear time \parencites{leo, marpa}.  Leo accomplishes
this by memoizing certain cases of right recursion that would
otherwise take $O(n^2)$.  Note that the set of LR-regular languages
contains the deterministic languages \parencite{lr-regular}.  GLL and
GLR both parse deterministic languages in linear time and unambiguous
languages in a worst case of $O(n^2)$, but both can potentially parse
some unambiguous languages in linear time.  GLR can parse in linear
time any language that its underlying LR automaton can parse.  For
example, Elkhound uses an LALR(1) automaton\parencite{elkhound1}.  The
grammars GLL can parse in linear time depends on its lookahead.
Spiewak's implementation\parencite{spiewak} and Scott and Johnstone's
description\parencite{gll1} use one token of lookahead and thus parse
LL(1) grammars in linear time.  All of these sets are much smaller
than the LR-regular languages.

The second consideration involves constant factors.  There are
conflicting reports in the literature about how different parsing
algorithms compare to each other.  The most extensive comparison of
modern parsing algorithms is in \textcite{antlr4}, where the authors
benchmark their ALL(*) algorithm against GLR, GLL, packrat/PEG, and
hand-written recursive-descent implementations as well as some others.
On their test case, Java code, all of the limited parsers were much
faster than the general parsers, by orders of magnitude in some cases,
with Rascal's GLL parser performing particularly poorly.  They don't
directly compare their algorithm against an Earley parser but say that
\textcite{tomita1985efficient} ``shows GLR to be 5x-10x faster than
Earley.''  It's unclear where this penalty comes from, and it should
be noted the comparison may no longer be valid because Tomita compared
the algorithms before Leo published his modifications that make Earley
run in linear time on LR-regular grammars.  This still seems to be the
general impression about Earley versus GLR, but I haven't seen any
evidence backing it up outside Tomita's work.  They suggest that based
on disabling the caching of lookahead DFAs, ``This performance is in
line with the high cost of GLL and GLR parsers that also do not reduce
parser speculation by memoizing parsing decisions,'' which seems to
suggest part of the problem the general parsers were experiencing was
O($n^2$) performance because of local ambiguities, not constant
factors, while in their testing, ALL(*) ran in linear time on all of
the grammars they tried.  [TODO: Is this a consequence of the general
parsers only handling a too-limited set of grammars in linear time?  I
don't know if any of these Java inputs fall into LALR(1), for
instance.]  However, part of the problem could easily be constant
factors, with even a linear GSS being slower than a standard stack,
for instance.  They note, ``Of the GLR tools, Elkhound has the best
performance primarily because it relies on a linear LR(1) stack
instead of a GSS whenever possible.  Further, we allowed Elkhound to
disambiguate during the parse like ALL(*).''  As ANTLR generates
recursive-descent parsers, this suggests that a parser combinator
approach that uses metaprogramming could be competitive.  Also, like
Elkhound, a GLL parser could use standard recursive-descent when
dealing with nonambiguous inputs for better constant factors.
Assuming that GLL has better constant factors than Earley, it's almost
certain that there's no global answer to which runs faster because it
will depend on the grammar and the input.  Beyond that, it's quite
possible the issue is even more complex because the different memory
consumption of the two algorithms will affect their performance as
well.

Beyond performance, other factors in algorithm choice are
implementation complexity, ease of debugging, compatibility with
parser combinators, and compatibility with DDGs/L-AGs.  First, while
none of the general CFG algorithms could be called simple, GLL is the
easiest because it works almost like a recursive-descent parser.  GLR
is strictly harder because it adds the difficulties of LR parsing,
like building an LR automaton and the bottom-up depth-first flow, on
top of the graph-structured stack and shared packed parse forest
needed for GLL.  Earley with Leo's modifications is quite complicated,
probably more so than GLL.  The original versions of both GLR and
Earley have theoretical problems with producing parse trees, and
fixing those problems has made the algorithms more complicated and, in
GLR's case, created a plethora of different versions.  Second, LR
parsers are notoriously difficult to debug, partly because they work
bottom-up.  GLL parses depth-first top-down and Earley parses
breadth-first, both of which are easier to follow.  Third, there's at
least one existing implementation combining GLL with parser
combinators, \textcite{spiewak}, and a paper describing a theory for
combining them with Earley's algorithm, \textcite{earley_combinators}.
As far as I know, there's no work on implementing parser combinators
with bottom-up algorithms like GLR.  Finally, GLL should be easy to
combine with DDGs/L-AGs because of the top-down left-right information
flow in both GLL and DDGs/L-AGs, but the original DDG implementation
uses Earley's algorithm\parencite{yakker1} so that should be doable as
well.

% Earley is the clear runner-up here, since there exist implementations
% of it using parser combinators and efficient theoretical developments
% of the algorithm that include all the key features I want like SPPF
% creation.  The original DDG paper \textcite{yakker1} implements DDGs
% on top of Earley's algorithm.


\subsection{Error Recovery}
\label{sec:errors}

For error handling I'm using Richter's non-correcting error recovery.
When parsing binary data, one major problem with finding errors is
that the point where the parser fails is often not the point where the
input or parser went wrong.  Non-correcting error recovery brackets a
range where the error may have occurred, making it easier to find
errors.  It's also more suited to the kinds of errors in binary data
in general, which are usually not like typos or other small isolated
errors but large blocks of broken data, and to Python's interactive
debugging.  It uses the suffix grammar and the reverse grammar of the
original input grammar.  While deterministic grammars are not closed
under suffix (I don't know about reverse), CFGs are, and because I'm
using GLL, I can use the same algorithm for both normal parsing and
error-handling.


\subsection{Architecture}
\label{sec:architecture}

The reason to write my own library in Python rather than using
Libmarpa CFFI bindings is to support Python variants.  CFFI only works
to allow calls from the interpreter level to C functions, not RPython
to C.  Meanwhile, Jython and IronPython don't, as far as I know, have
anything to allow calling into C at all.  Writing it in Python with
metaprogramming to convert it to RPython will make it
platform-independent. Also worth noting is that after seeing the
performance benchmarks for Scala parser combinators with macros and
staging, I'm skeptical that it's necessary to write a parsing library
in a low-level language to get good performance, and that the kind of
code that's fast doesn't change much from interpreter to interpreter:
it's all simple, imperative-style code with obvious optimizations.


\section{Implementation Order}
\label{sec:implementation_order}

\begin{enumerate}
\item GLL combinators.
\item Non-correcting error recovery, for debugging.
\item Tests.
\item Performance benchmarks.
\item Macros, staging, or other metaprogramming for performance.
\item Data-dependent grammar functionality, in some as yet TBD order.
\item Unparsing.
\end{enumerate}


\section{Major Implementation Choices}
\label{sec:implementation_choices}

\subsection{GSS Implementation}
\label{sec:gss_implementation}

\textcite{afroozeh_izmaylova} improve the performance of GLL with a
different implementation of the GSS that places information on the
edges as well as the nodes to avoid creation of duplicate nodes that
represent recalculating the same nonterminal at the same input
position in different parse paths.  \textcite{spiewak}'s version of
GLL returns a stream of results, and because it doesn't share results
across different parse paths, the recalculation of results is critical
for its correctness.  In other words, Afroozeh and Izmaylova cut out
exactly the inefficiency that allows a GLL parser to not use an SPPF.
If you ensure that a particular nonterminal is only calculated once
for a given input position, though, you \emph{have} to share parse
results across different paths, which means you have to use an SPPF.

Spiewak's implementation has one other inefficiency compared to both
Afroozeh and Izmaylova's and \textcite{gll2}'s: he moves the code from
\texttt{create()} function into \texttt{add()}, and thus his GSS
contains nodes corresponding to terminals as well as nonterminals.  In
the case of Afroozeh and Izmaylova's example, parsing ``\textbf{aac}''
with $A ::= aAb | aAc | a$, Scott and Johnstone's GSS contains four
nodes.  Spiewak's creates nine nodes: because Spiewak's algorithm
breaks each nonterminal into its individual terminals instead of GLL
blocks, there are nodes for each position in the input, 0, 1, and 2,
not just 1 and 2 as in Scott and Johnstone's.  Likewise, there are
three nodes for each input position because there are three
alternatives for $A$, including the terminal $a$.

Afroozeh and Izmaylova's other optimizations also result in clear
performance gains, both moving the sets $\mathcal{U}$ and
$\mathcal{P}$ from global hash tables to local hash tables on the GSS
nodes and eliminating duplication checks during node creation in the
GSS and SPPF.  (Note that eliminating the SPPF duplication check also
requires using their GSS.)  Moreover, in Python there's no good native
linked-list data type that would work for the GSS layout described in
\textcite{gll2}, only the memory-intensive and
unnecessarily-doubly-linked \texttt{collections.deque}, while Python's
hash tables are extremely optimized.  In their implementation, GSS
nodes are labeled with a nonterminal (a class instance) and an index
into the input and have links to their outgoing edges and two local
hash tables associated with $\mathcal{U}$ and $\mathcal{P}$, the
former containing descriptors as a grammar slot plus an index and the
latter containing an index.  GSS edges are labeled with a grammar
slot, i.e. a nonterminal/class instance and an integer representing an
index into a nonterminal.  Each edge has a link to a GSS node and an
SPPF node.


\subsection{Return an iterator over parse trees or a shared packed
  parse forest.}
\label{sec:iterator_sppf}

\textcite{spiewak} chose the former option for his GLL combinators in
Scala, but it has a couple of problems.  First, the obvious
implementation in Python uses generators/coroutines, which has other
problems discussed in Section
\ref{sec:functions_coroutines_continuations}.  It also requires
freezing the parser state as long as the iterator isn't exhausted,
which is memory-intensive and would require some manual cleanup to
close running coroutines.  Second, it means that changing the
implementation of the parse forest almost certainly involves changing
the implementation of the parser itself because the parser state is
entangled with its output.  Third, iterating over subtrees as well as
alternatives trees will require nested iterators, i.e. an iterator
that returns another iterator, which gets confusing and requires some
doubly-suspended state.  \textcite{grune_jacobs} bring up another
problem in what they call the producer-consumer model: anyone wanting
to use the parse trees will have to analyze multiple trees to figure
out how they differ to figure out what to do with them, which means
holding multiple trees in memory at the same time.  (Note that it's
easy to build an infinite iterator so that method should be able to
handle infinitely-ambiguous grammars.  Avoiding analyzing tree-by-tree
will require users to understand the SPPF and a good API for analyzing
it without generating trees.)  They point out that coroutines can
reduce some of these problems by putting the parser and the consumer
of the parse tree on an equal footing.  \textcite{moura_ierusalimschy}
proves that asymmetric coroutines are as powerful as symmetric ones,
so it ought to be possible to implement this in Python, but I suspect
the implementation would involve creating a trampoline that passes
data between the producer and consumer coroutines, and I'm not sure if
the added complexity is worthwhile, especially given it would involve
a performance hit.

Spiewak
\href{https://github.com/djspiewak/gll-combinators/issues/24#issuecomment-59147169}{observes}
that he hasn't, ``been able to cleanly solve the problem of how to
encode a proper SPPF in a functional, \emph{combinatorial} style.''
[This may be related to the problem of representing graphs in a
functional style and the possibility of cycles in the SPPF.]  I'm more
concerned about performance and usability than functional purity, so
I'm running with the SPPF.


\subsection{Tree and SPPF implementations}
\label{sec:tree_sppf_implementations}

The standard implementation of a parse tree in Python is nested
lists/tuples.  (Some people call this a ``rose tree'', ``k/m/n-ary/way
tree,'' or ``multiway tree,'' usually to distinguish it from trees of
fixed arity.)  Moving from trees to SPPFs introduces two complexities.
First, there needs to be a new type of node, which I will call ``or
nodes,'' to represent cases where the same subsequence of the input
has multiple possible derivations.  Second, subtrees need to be
shared, and to bound space complexity by $O(n^3)$, the SPPF has to be
binarized\parencite{billot_lang}, so that all of the original nodes,
which I will call ``sequence nodes,'' have only two children.  As
\textcite{billot_lang} note, representing the sequence nodes with
linked lists provides the necessary binarization.  In effect, this
change puts a tree into
\href{https://en.wikipedia.org/wiki/Left-child_right-sibling_binary_tree}{left-child
  right-sibling binary tree} form and allows the necessary sharing in
an SPPF.  Scott and Johnstone use the term ``symbol nodes'' to refer
to the head of these linked lists, ``intermediate nodes'' to refer to
the other nodes in the linked list, and ``packed nodes'' to refer to
what I call ``or nodes.''

Unlike for a single parse tree, when building a shared packed parse
forest it's necessary to label the nodes so that different branches of
the parse traversal share subforests.  Traditional parser combinators
don't provide any natural node labeling because of the absence of
labeled nonterminals.  Moreover, GLL needs labels to merge different
branches of the parse in the GSS.  There's an natural labeling scheme
for parse trees based on partitioning the input that Scott and
Johnstone use but never explicitly explain.  Each terminal or
uninterrupted sequence of terminals has a starting and ending offset
so the corresponding leaf node can be labeled with (terminals,
starting\_offset, ending\_offset).  The leaf nodes read in offset
order must cover the entire input, and in fact reading them in order
in the absence of semantic actions will return the original input.
The rest of the tree nodes represent partitions further subdivided
into subpartitions labeled with nonterminals, with the root node
corresponding to (start\_symbol, 0, length(input) - 1) (zero-indexed).
Because parser combinators don't have nonterminal labels, Spiewak uses
parser identity instead.  Like Spiewak, I intend to use parser
combinator class instances as node labels in GLL.  [TODO: discuss
Earley.]

However, I don't understand the relationship between Scott and
Johnstone's partition model and parses that don't consume all the
input.  The natural way to implement nondeterminism leads to returning
all incomplete parses as well as the complete parses.  Both Spiewak
and Hutton and Meijer mention this natural ambiguity.

\begin{quote}
  It is also worth noting that we do not allow \texttt{Success}(es)
  which have failed to consume the entire input stream.  We will
  actually receive a \texttt{Success} every time a \texttt{Parser}
  reduces successfully. While this is technically correct (reflecting
  the ambiguity between greedy and non-greedy matching), it is almost
  never the desired semantics.\parencite{spiewak}
\end{quote}

Scott and Johnstone's constructions of parsers from recognizers seem
to rely on greedy matching, because their partitioning scheme only
works with matches of the full input.  They also restrict packing to
nodes that share the same partition, ``Nodes can be packed only if
their yields correspond to the same portion of the input
string''\parencite{sppfs_from_recognizers}.  This doesn't seem to work
in cases with partial/non-greedy matching because every nontrivial
node will have partial matches that don't cover the same partition.

A final note on the greedy-matching issue is that the obvious
implementation of nondeterminism applied naively leads to the wrong
recognizer, because a partial match will report success even if the
whole string is not matched.

In \textcite{gll2}, they propose implementing the GSS and SPPF with a
combination of arrays and linked lists, arrays where GLL requires
constant-time lookup and linked lists to save space where $O(n)$
lookups won't affect GLL's asymptotic behavior and trading time for
space is acceptable.  As both the GSS and SPPF are graphs, this
approach amounts to representing them with adjacency matrices and
trying to used linked lists to reduce the matrices' space consumption.
It depends on knowing the grammar and input to determine utilization
counts, so an actual implementation would need heuristics to make
these same judgments.  In a later paper, however, they say this
implementation is not practical:

\begin{quote}
  One approach is to use a table of all possible elements, implemented
  using sparse matrix techniques [12] in which table rows are
  allocated on demand.  This technique is impractical for realistic
  sized problems, since even with sparse optimisations, high memory
  consumption leads to swapping during the parse.\parencite{gll3}
\end{quote}

From my own experience with Construct, I find this \emph{entirely}
unsurprising.

In \textcite{gll3}, they tried a different implementation:

\begin{quote}
  The natural object-oriented approach to constructing these data
  structures defines a class for each kind of element using \emph{k}
  individual reference fields to represent \emph{contains k-of}
  relationships for small \emph{k} and some sort of collection class
  (in Java, the HashMap generic) to represent \emph{contains many-of}
  relationships. Each element of the data structures then becomes a
  separate object, allocated on the heap.
\end{quote}

This also consumed too much memory because of the object overhead when
creating millions of objects, as well being too slow because of
garbage collection.  They work around these problems with an object
pool, which of course amounts to doing manual memory management with
all of its problems.

The
\href{https://github.com/yitzhakm/yakker/blob/master/src/history.ml}{ADT}
that Yakker uses to represent its SPPF translated into pseudo-Python
(using \texttt{typing}) looks something like,

\begin{lstlisting}
from collections import namedtuple
import typing

A = typing.TypeVar('a')
Label = typing.TypeVar('Label')

Root = namedtuple('Empty', 'label: Label')
       | namedtuple('Root', 'info: Info[A, Label]')

Info = namedtuple('Info', 'label: Label, v: A, branchings:
                  List[Branching[A, Label]]')

Branching = namedtuple('Branching', 'root: Root[A, Label]')
            | namedtuple('Branching', 'root: (Root[A, Label], Root[A, Label])')

\end{lstlisting}

The Yakker SPPF is parameterized over the values the tree holds
(\texttt{A}) and the type of the labels (\texttt{Label}), which in the
\textcite{gll2} SPPF are symbols (nonterminals and terminals) and
integers corresponding to input positions.  The list of
\texttt{Branchings} corresponds to the or nodes; note that the
out-degree of the list and packed nodes can be more than two.  The
\texttt{Branchings} themselves are the sequence nodes and have
out-degree of at most two.  Thus, the Yakker SPPF nodes embed both
packing and normal branching in the same data structure, as each node
contains a list of pairs of children.  Yakker uses a weak hash table
containing links to all the partial trees in the forest, though the
implementation seems to use the hash table like a set, with a function
that checks directly if a partial tree is in the table and returning
it if so.

[TODO: discuss Marpa's.]

There's one possible other implementation of the SPPF about which I
know little called parse-forest grammars, introduced on pp. 91-93 of
Grune and Jacobs.  They have production rules labeled almost
identically to Johnstone and Scott's SPPF, a nonterminal, a starting
position, and a length (which is isomorphic to using the ending
position).  I personally don't see how any of the supposed advantages
Grune and Jacobs list for them are good for writing practical parsers
and the API they'd provide would be very unintuitive for people used
to conventional parsers, but they're another possible internal
implementation.

My own experience and the results of Johnstone and Scott suggest that
reducing the memory overhead of the SPPF is essential.  Also, all the
nodes must be immutable to make sharing safe.  The sequence nodes must
be implemented as a linked list, and the only linked list
implementation available in the standard library is
\texttt{collections.deque}.  An empty deque takes up 616 bytes
according to \texttt{sys.getsizeof}, and while it doesn't scale
quickly, increasing to 1128 bytes somewhere between 32 and 64
elements, for an SPPF which will have many small linked lists it's
completely unsuitable.  Deques are also mutable.  The two obvious
options for implementing a linked list in Python are to use a class or
tuples for nodes.  Tuples take more memory for some reason, 64 bytes
for a two-element tuple compared to 56 bytes for a two-element class
instance with \texttt{\_\_slots\_\_}, but are superior in all other
respects because they guarantee immutability, which is otherwise
impossible to enforce programmatically at the Python level, are much
faster to initialize on CPython, and contain some useful methods
implemented in C like \texttt{\_\_hash\_\_} and \texttt{\_\_eq\_\_}.  For the
or nodes, tuples are the obvious correct implementation.  Frozensets
are also immutable and have variable arity, but they have more memory
overhead for no gain, because hashing parser instances in the
alternation combinator should ensure that no or node is ever created
containing duplicate subforests.

The obvious implementation for looking up nodes by labels in Python is
a dictionary i.e. a hash table.  Because hash tables have worst-case
$O(n)$ lookup costs, parsing can take longer than $O(n^3)$ in the
worst case, but the average case should remain the same.  Python's
hash functions are well optimized to avoid collisions, but if
necessary I could look at implementing my own hash functions if for
some reason the generic implementations don't work well for parsing.
This layout is an adjacency list representation of a graph with a hash
table (dictionary) as the outer data structure and linked lists and
arrays (tuples) as the inner data structures.  It's similar to
\href{https://www.python.org/doc/essays/graphs/}{van Rossum's
  suggested Python graph implementation} except that instead of
holding labels, the inner data structures hold references to each
other.  Holding references instead of labels saves memory because even
a 64-bit pointer only consumes 8 bytes while a label made up of Python
objects consumes much more and because, as Jeffrey Kegler pointed out
to me, after the SPPF is constructed the structure holding labels
pointing to nodes is no longer necessary and thus can be freed.

One possible approach for the labels is use a three-tuples of a
combinator instance and two integers, for sequence nodes, and
two-tuples of a combinator instance and an integer, for or nodes, as
keys.  Another is to use chained dictionaries, with the first
dictionary containing combinator instances as keys and containing one
or two further levels of dictionaries, depending on whether the
combinator is a sequence or an alternation, with integers as keys.  In
their testing on the GSS, \textcite{afroozeh_izmaylova} discovered
that having one giant hash table performed worse than having multiple
smaller hash tables, which argues in favor of the latter choice.  In
my microbenchmarks building nested dictionaries or dictionaries with
tuple keys and then doing lookups, tuple keys were about three times
faster on CPython and nine times faster on Jython, while on PyPy
nested dictionaries were about two times faster.  It will require
profiling to decide between these two possible implementations.

One problem with using tuples as the nodes is that on CPython, they
can't be weak-referenced.  This makes it impossible to use Yakker's
weak hash table approach to ensure that partial subforests in branches
of the parse that ultimately fail are garbage-collected during parsing
on CPython.  [I'm not clear on why Yakker needed to use this approach,
since Kegler claims that Earley doesn't build partial forests that
aren't part of the final forest.]  This should only matter when
parsing inputs that are locally but not globally ambiguous, and it's
not clear how important the memory savings are.  This will also
require profiling.  If it turns out to be an issue, I wrote a linked
list node as a simple Cython extension type that can be
weak-referenced.  However, there's no good solution for the or nodes:
an array is obviously the right data structure for them but there's no
weak-referencable immutable array available in CPython, so I'm stuck
either reimplementing tuple with weak reference support in Cython or
using an ugly list subclass with the mutation methods overridden.
PyPy and Jython tuples can be weak-referenced so don't have these
problems.

The clearly-fastest and smallest implementation on CPython with the
best encapsulation is to use Cython to write a specialized SPPF
object, with the exact methods needed for building it.  However, this
solution doesn't work for any other Python implementation.  Given the
problems that \textcite{gll3} reported with their Java implementation,
it may also be necessary to push the SPPF data structures down levels
of abstraction to save memory on all Python implementations.  It's not
clear how one does this on PyPy, though it probably involves RPython.
Any encapsulation has a performance overhead on CPython, Jython, and
presumably IronPython because the parser will have to call methods
written in Python, so they may also need lower-level implementations
of the data structures for speed.  On the whole, I'm convinced enough
of the advantages of encapsulation, particularly the possible need to
reimplement data structures at lower levels of abstraction, that I
will write the first version in Python with as much encapsulation as
possible and then use profiling to decide what changes, if any, are
necessary.  This Python version should serve as a good starting point
for reimplementations in Cython and RPython, if necessary.

\subsubsection{Labeling the end nodes of linked lists}
\label{sec:end_node_labels}

In a canonical linked list, the end node's reference to the next
element contains a special element, in this case an empty tuple,
signaling it's the end of the list.  Unfortunately, when it comes to
labeling, the left and right extents of the end node are the same as
those of the object it contains.  Scott and Johnstone resolve this
issue by making their end nodes contain two non-node objects, but this
complicates the implementation of the linked list.  I'm keeping the
list implementation simple by making sure that when an object is
contained by an end node that that end node is represented by the
label in the SPPF.  (Otherwise, the SPPF will contain many different
end nodes, each referring to the same underlying object.)   This is
awkward and has a memory cost, but the other alternative is also awkward.

\subsubsection{Cycles in the SPPF}
\label{sec:sppf_cycles}

Infinitely-ambiguous grammars can create SPPFs with cycles in them.  A
natural case arises when trying to invert the grammar for a
programming language that ignores whitespace, since all
infinitely-many possible insertions of whitespace are valid.  In a
language with eager evaluation like Python, it's impossible to
represent cyclic data structures without mutation.  However, my
design, a hash table pointing to singly-linked lists, depends on the
immutability of the linked lists to implement sharing.  I'm ignoring
this problem for now (Jeffrey Kegler notes that no one has ever used
the cycle-handling code in Marpa), because infinitely-ambiguous
grammars are of little use for practical parsing and will probably
only come up during automatic grammar transformations (inversions,
reversals, etc.).  If and when the issue arises, there are three
possible ways to handle it: create a lazy singly-linked list data
structure, detect cycles in grammars and rewrite them to be
cycle-free, or use a cycle-generating function like Haskell's.

% <ceridwen> Idiosyncrat, How does Marpa handle the case of infinitely-ambiguous grammars in the SPPF, data-structure-wise?
% <Idiosyncrat> With great difficulty. :-)
%  Basically, you just keep track of everywhere there might be a cycle, and prevent it or make sure it is harmless.
%  In the future, I will either simply ban cycles, or eliminate them in the grammar rewrite ...
%  and eliminate the cycle-handling code from Libmarpa.
%  I'm writing the Marpa Book as if it did not handle cycles.
%  The great problem with previous parsers is they've been under-engineered ...
%  Arguably, I erred in the opposite direction, and I will be backing some of that out.
%  If you use the SLIF, it automatically bans cycles.
% <ceridwen> Apparently this is a particularly painful problem, so it's no surprise I've been struggling with it.
%  I'm most concerned about the case where an automatically-generated grammar creates an infinitely-ambiguous situation---an obvious case arises when trying to invert the grammar for a programming language that ignores whitespace, since all infinitely-many possible insertions of whitespace are valid.  The immediate problem is that I've been planning on using a hash table + immutable linked-list implementati
%  on for the SPPF, but this does not play nicely with cycles in a language like Python with eager evaluation.
%  My temptation is to stick with the design I have for now and then, when handling cycles, use lazy evaluation, but I don't know if that's going to get me into massive amounts of trouble.
% <Idiosyncrat> Cycles can be detected ...
%  it's yet another use for a transitive closure algorithm.
%  That's what the SLIF current does -- it detects a cycle and prints a diagnostic.
% <ceridwen> Since I want to deal with arbitrary CFGs and in some cases CSGs, I guess it's possible to automatically eliminate a cycle?
% <Idiosyncrat> Consider what it means to handle an infinite cycle in the evalution phase.
%  That is, you cannot actually perform an infinitely long evaluation.
%  The usual thing to do is, when there is an infinite cycle, to refuse to do any processing.
%  Currently Libmarpa will follow rules which involve infinite cycles for a length of 1 -- that is, until they repeat.
%  Nobody every found a use for this.
%  If anybody ever does find a use for this, it is better handled as a grammar rewrite.
%  The cases where Libmarpa follows infinite cycles is where they, in addition to infinite cycle, also reach a terminal ...
%  If a cycle is just simple A -> A where A is a non-terminal, then it's just an infinite loop, and Marpa does nothing with it.
%  But if you have A ::= x | A ...
%  that is an infinite cycle, but there is also a way to reach the terminal <x>, so Libmarpa (with the right options set) will do that
%  and silently ignore the cycle.
%  Most users are happiest if you just treat A ::= A | x as a fatal error.
% <ceridwen> Infinite cycles like A -> A can be automatically eliminated without changing the language of the grammar, so I'm assuming that when I'm doing automatic grammar processing for inversion or error-handling, I will have to rewrite the grammars to avoid those.  The terminal cases are the ones that are tricky.  For inversion, they have to be disambiguated at some point.
%  I was planning to have one ex-post-facto disambiguation system that runs during/after parsing, but I'm not sure that when dealing with an automatic inversion this is superior to forcing users to redefine the grammar to make it unambiguous.
% <Idiosyncrat> To handle them as a grammar rewrite, just follow the paths, renaming symbols so they are unique per path ...
%  paths end when you reach a terminal, in which case you use the path ...
%  or when they cycle, in which case you do not use the path.
% <ceridwen> For error-handling, it's not immediately clear to me what's right, though it's also not clear to me that the reverse grammar can even generate a cycle if one doesn't exist.  (I'm hoping not, honestly, but I haven't proven it yet.)
% <Idiosyncrat> In some grammars this can lead to an explosion in the number of symbols and rules, but that's unavoidable.
% <ceridwen> Right.  Can it be bounded?
% <Idiosyncrat> It's bounded, yes.
%  But I think it may be exponential if the cycle is a plex --
%  something like A :: = B | C | D ; B ::= A | C | D; C ::= A | B | D; D ::= A | B | C
%  they are actually some of these in Marpa::R2's test suite.
%  Frankly, I'd love to have all the time I spent on them back to devote to other things. :-)
%  Unfolding all the symbols and rules of a plex is (I think) exponential in the number of symbols ...
% <ceridwen> Your advice, then, would be to stick with my current SPPF design and defer the cycles issue, possibly indefinitely?
% <Idiosyncrat> so it is easy to automatically create a grammar which takes an impractical amount of time to rewrite.
%  Y. E. S.
%  That's exactly my advice.
% <ceridwen> It's useful to know that no one's ever used them.  Thanks for the advice and notes on grammar-rewriting---that's probably a better solution than using lazy evaluation to circumvent the problem.
% <Idiosyncrat> Yes, no users AFAICT.  The best evidence is when I quietly yanked support for them which I created the SLIF.  No complaints.  None.
% In fact, no evidence that anybody even noticed the absence of support of cycles.


% https://www.python.org/doc/essays/graphs/

% Both trees and forests are fundamentally directed graphs, normally
% acyclic but in the case of infinitely-ambiguous grammars cyclic.  In
% Python, one possible graph implementation is a dict of lists, where
% the lists contain the labels of other nodes.  By using the labels
% discussed above as the labels for the nodes, it's possible to
% represent a parse tree as a digraph where the values of leaf nodes are
% Python objects representing terminals and the values of non-leaf nodes
% are lists of either pairs of integers, the direct labels for other
% nodes, or single integers representing the subpartitions for that
% node.  Eliding the terminal and nonterminal symbols for clarity, for
% instance:

% \begin{lstlisting}
% (1, 8) : [(1, 2), (2, 5), (5, 8)]
% (1, 8) : [2, 5]
% \end{lstlisting}

% The superfluous integers correspond to the "repeated dimensions" that
% Johnstone and Scott describe in \emph{Modelling GLL Parser
%   Implementations} (2010).  The latter implementation obviously takes
% less memory but node operations will be slower because correct
% offset-pairs will have to be generated from the partitions when
% they're needed.

% The SPPF can have almost an identical implementation to the parse
% trees themselves because it's also fundamentally a digraph.  There are
% only two differences: nodes can have more than one parent,
% corresponding to subtree sharing, and there has to be a way to
% distinguish packed nodes from normal nodes because they have different
% meanings.  In the binarized SPPF, Johnstone and Scott define normal
% nodes using two offsets, called "left extent" and "right extent", and
% packed nodes using a single integer, called "pivot."  In the
% partition implementation any node with only two children can be
% represented with a single integer, the division between the subtrees.
% A packed node is the result of combining at least two nodes, so a
% minimal binarized packed node looks like:

% Original nodes:
% \begin{lstlisting}
% (0, 4): [(0, 1), (1, 4)]
% (0, 4): [(0, 3), (3, 4)]
% \end{lstlisting}

% Packed node:
% \begin{lstlisting}
% (0, 4): [((0, 3), (3, 4)), ((0, 1), (1, 4))]
% \end{lstlisting}

% Binarized nodes:
% \begin{lstlisting}
% (0, 4): Packed(1), Packed(3)
% Packed(1): [(0, 1), (1, 4)]
% Packed(3): [(0, 3), (0, 4)]
% \end{lstlisting}

% A further observation that may or may not be related is that their
% node labeling scheme seems to include unneeded information.  A given
% parser started at the same position should always produce the same
% output, so all that's needed for a unique node label is (parser label,
% starting index into the input).  In fact, in one of their early
% papers, \textcite{brnglr}, they use this labeling scheme, rather than
% the later three-element labels.  I don't understand why, and if or how
% this might be connected with greedy matching.  



\subsection{Tree and SPPF API}
\label{sec:tree_sppf_api}

There's very little reason, as far as I can tell, to expose the
internal node labels to the user.

With a traditional recursive-descent parser, the definitions of the
nonterminals in the grammar and their corresponding functions provide
natural nodes for the parse tree.  In a parser built from combinators,
however, because the combinators don't intrinsically have nodes, it's
not clear how to build the parse tree.  As far as I can tell,
traditional parser combinators always use the sequence combinator to
build the tree out of nested lists/tuples, either by having it build a
flat list/tuple out of two or more alternatives like Construct's
Sequence (Struct, which makes an ordered map, is a variation on this)
or nested binary lists/tuplesx, like Spiewak's combinators.  Other
non-monadic combinator implementations follow the same general
approach: Hughes in \emph{Generalizing Monads to Arrows} observes,

\begin{quote}
  For example, it is fairly clear that a library for parsing should
  include a combinator to invoke two parsers in sequence, but there
  are many possible ways in which such a combinator might handle the
  two parsers' results.  In some early parsing libraries the two
  results were paired together, in others the sequencing combinator
  took an extra parameter, a function to combine the results.
\end{quote}

The clearest explanation I've found of the relationship between this
traditional sequence combinator and monadic combinators is in Vegard
\O ye's article (2012, \url{https://github.com/epsil/gll}): they
define an operation bind (this is the monadic bind) that takes a
parser and a function, applies the parser to the input, applies the
function to a success to get another parser, and then applies the
resulting parser to the success to get the final output.  They then
define the sequence combinator in terms of a double bind, passing the
list constructor as the function to bind; for a sequence combinator
with more than two elements, they use fold/reduce with list-append to
get a flat list rather than nested lists from one combinator.  The
Hutton and Meijer paper (\emph{Monadic Parser Combinators}) also shows
how to build a sequence combinator using bind and a concatenation
function for lists, in this case specifically to avoid nested tuples.
While the standard approach to defining monads, settled on by Haskell
and seemingly imitated by everyone else, is to define a monad in terms
of return (sometimes called result) and bind, one can also define
monads in terms of three functions, return, join, and map (sometimes
fmap), and then define bind in terms of join and map.

Writing monadic (or arrow-style) parsers in Python would be stupid, so
I need a sequence combinator and will follow this universal (as far as
I know) practice and define the nodes in the tree using it.  I can let
it take a constructor with which to build the parse tree with a
default constructor or expect that standard usage involves
transforming the parse tree with semantic actions.  While monadic bind
combines the choice of parse tree with the sequence combinator, my
understanding is that separating join and map separates the two, so
having a semantic action combinator following a sequence combinator
can simulate any choice of bind for monadic combinators.

Irrespective of the internal implementation, I also need to figure out
the interface the SPPF presents to the users.  The SPPF itself is hard
to understand so a direct implementation would make for a bad API.  My
best idea for handling this is to build a tree-centric API where users
interact with the SPPF as if it was a collection of trees.  One
obvious problem after creating the SPPF is how to allow user code to
examine individual trees.  Luckily, this problem has an obvious
solution: instead of returning a copy of an individual tree, I can
return an object that contains references to the correct nodes in the
SPPF.  However, implementing anything more than simple views on the
SPPF is \emph{hard}.  I still haven't come up with a better idea for
the tree API itself than Construct's: because trees are a recursive
data structure, allowing recursive references using Python's []
addressing provides a natural API.

That said, for users to deal with ambiguity in any efficient manner
will probably require users to both understand the SPPF and have
direct access to it; I have no idea how to implement such an API or to
integrate it with the hopefully-simpler tree-centric API.

How semantic actions interact with the SPPF presents some thorny
issues.  Semantic actions are essential, as aside from needing them to
emulate the power of monadic combinators, they're required for
directing parsing with previously-parsed input, essential in many
common parser tasks like ignoring whitespace, one of the better reasons
for using a top-down parser in the first place, and possibly important
in performing disambiguations while the parser is running.  Because
I can't figure out how to transform the SPPF as if it was a collection
of trees, though, without brute force and maybe not even then, I still
haven't settled on how to set semantic actions up.  I have some
incompletely-realized partial ideas:

\begin{itemize}
\item Higher-order functions on trees: filter, map, reduce.
\item A higher-order function that converts a function acting on trees
  to a function acting on SPPFs.  I don't know how to write it, though.
\item The visitor pattern and other kinds of traversals with functions
  acting on the tree/SPPF.
\end{itemize}


\subsection{Mutability versus persistence}
\label{sec:mutability_persistence}

There are two different approaches to the data flow through the
parser, the functional-immutable style where functions, or in this
case, coroutines, create objects and return them and the mutable-state
style where a mutable object is passed into a function or coroutine,
which then mutates it and returns nothing.  The choice of which style
to use can be made on a per-object basis, but because in most
languages including Python functions can only return one object, and
in the case of Python coroutines can only yield and accept through
send() one object, functions that need to return more than one object
have to aggregate all the objects they need to return into one object.
Handling the aggregation and disaggregation for parsers is an example
of the monadic pattern, as even recognizer combinators need to return
a Boolean representing whether a given input is in the language
generated by a grammar and also a stream object representing the
truncation of its input.  Real parsers always need to return the same
Boolean, a stream, and a parse tree.  Other patterns are possible:
Construct uses a Python stream, which is a mutable object, and thus
doesn't need to return a stream.  An empty parse tree or some kind of
null result (None in Python) can represent failure, but this is a bad
idea since it provides no information about where the failure occurred
or what happened.

In traditional object-oriented recursive-descent parsers, the whole
parser would be one class and mutable objects could be handled as
shared state.  As it is, since my combinators are classes, I have to
pass mutable objects instead.  This isn't the worst thing in the world
since passing objects will avoid self look-ups, helping performance,
and is explicit rather than implicit.  Several of the implementations
I've looked at do similar things.  Spiewak's GLL combinators use a
functional-immutable style for the tree, returning a list in the
simplified implementation and a stream/iterator in the real
implementation, but an OO class for the trampoline.  Scott and
Johnstone's example GLL implementation uses an OO class for the parser
with a mutable shared SPPF.  Jim and Mandelbaum's transducers for
data-dependent grammars also use a mutable SPPF.

For GLL, the combinators could potentially return success/failure, the
GSS, the SPPF, the stream, and the pointer.  Mutable objects only need
to be passed in when starting a new coroutine instance and don't need
to be yielded or passed via send() because every coroutine will
already have access to them.  The advantage of mutability is speed,
and its main problem is that it always has more potential for creating
hard-to-isolate bugs.  Unfortunately, Python is simply not designed
for the functional-immutable style and obviously has no optimizations
in the compiler/interpreter for handling immutable object creation for
non-built-ins.  Thus, the functional-immutable style with an
encapsulated Python object will be many times over too slow.
(\url{http://www.valuedlessons.com/2008/03/why-are-my-monads-so-slow.html}
gives some numbers suggesting how much too slow.)

For the SPPF, the only way to implement sharing in a functional
fashion is to pass a memoized cache and discard it after building the
SPPF.  Otherwise, combinators in different branches of the parse won't
be able to share subtrees.  However, a memoized cache itself is most
of the way to an SPPF, so there's no real reason to use the former in
place of the latter.  The SPPF is so large (I'm estimating a factor of
>100 times over the input, and that's optimistic) that recreating it
in every parser is wildly impractical, so it has to be mutable.
However, there's a further problem with building the SPPF as a mutable
object passed among combinators that Jim and Mandelbaum describe in
\emph{Delayed Semantic Actions in Yakker}: nodes that get added to the
SPPF during branches of the parse that eventually dead-end will
continue to exist in the final SPPF.  The solution they use in OCaml
is to use a weak hash table.  The best option in Python is similarly
to use a weak dictionary.  This creates several knock-on effects.
First, strong references to the nodes in the SPPF have to be stored in
the SPPF \emph{somewhere} or else the whole structure will get
garbage-collected, and the only reasonable place to put them is in the
nodes themselves, thus a weak dictionary of objects that hold
references to other nodes.  Second, most Python built-in types and
subclasses thereof are not weak-referencable in CPython.  Subclasses
of lists and dicts can be weak-referenced, while lists and dicts
themselves can't.  (None of this behavior is defined for all
implementations, though PyPy seems to follow CPython here.)  Changing
from subclasses of tuple to list has memory and speed penalties, but
the alternative to using Python's weakref module is doing manual
object destruction myself, and that's just awful.  Third, to keep
nodes alive while building the SPPF I need strong references outside
it, which means the combinators need to return strong references.
Fourth, terminals have to be enclosed in some kind of
weak-referencable proxy object.

That said, while the SPPF has to mutable, the nodes making it up can
be immutable.  There are advantages and disadvantages to both
approaches.

\begin{itemize}
\item Mutable nodes make it easier to transform trees and reduce the
  memory overhead of transformation operations since they avoid
  copying.  How common is it to need to transform a parse tree,
  though?  For writing a compiler/interpreter or a converter for a
  binary data format, for instance, the parse tree is used to build
  another kind of object, not mutated in-place.
\item Immutable nodes prevent a broad class of potentially
  hard-to-find bugs related to mutating a tree in one node in an SPPF
  and causing changes that propagate to other trees incorrectly.  This
  is a particular problem with semantic actions where users shouldn't
  need to understand the particulars of the parsers or the SPPF.
\item Immutable objects could provide memory advantages, but Python's
  implementation makes this difficult.  There's no built-in immutable
  mapping type, and tuples can't be weak-referenced.  To get the
  memory savings, I'd either have to give up speed by using
  composition and methods implemented in Python to enclose tuples in a
  weak-referencerable object or use Cython to implement my own types
  in C.
\item Immutable containers are hashable, which is essential for any
  kind of elegant way of handling which packed nodes to traverse in a
  tree view of an SPPF and useful for implementing packed nodes as
  frozensets, gaining the automatic ability to avoid duplicates when
  packing.
\end{itemize}

Spiewak passes the trampoline that contains the GSS as a mutable
object for ``convenience and efficiency,'' and I'm leaning towards
following his lead.  For parsing, I'm passing the stream as an
immutable object.  That leaves only success/failure, the pointer, and
a node reference as immutable objects I need to create in each combinator.

\begin{itemize}
\item GSS: mutable, passed
\item SPPF: mutable, passed
\item Stream: immutable, passed
\item Success/Failure: immutable, returned
\item Stream pointer: immutable, returned
\item Node: immutable, stored in SPPF and returned
\end{itemize}

There is one other possible way to circumvent the bad performance of
the functional-immutable style, which is to use compilation/code
generation to eliminate the intermediate data structures.  This may
turn out to be the best option, and in the end is almost certain to be
the fastest, especially if I go all the way to a two-stage compiler
that compiles Python to Cython and Cython to C.  At the moment, I
don't understand it well enough to enumerate its advantages and
disadvantages.


\subsection{Functions versus coroutines versus continuations}
\label{sec:functions_coroutines_continuations}

% TODO: rewrite this section

\subsubsection{Outer-Loop Trampoline}
\label{sec:trampoline}

In Python, the trampoline has to run as the outermost loop and call
the coroutines because Python coroutines are asymmetric, they can only
return control to their callers with yield.  If I instead try to pass
the trampoline through the combinators, there's no way for them to
pass control to the trampoline.  Conveniently, GLL is organized with a
dispatch function such that every parser returns control to it after
finishing execution, which is my trampoline.


\subsubsection{Yield from}
\label{sec:yield_from}

The main problem with "yield from" is that it's only available on
Python 3.  Also, experimentation suggests that as of 3.4, using "yield
from" still hits the maximum recursion depth which means that "yield
from" still adds a stack frame for each call, and Python has a small
stack limit.  On the other hand, without "yield from", Python's
coroutines are not stackful (see Moura 2009, "Revisiting Coroutines"),
which means they may not be as powerful as one-shot continuations and
might not be powerful enough for GLL.  That said, I know that "yield
from" is internally implemented with a trampoline, and the existence
of Eby's "fiendishly clever" trampoline implementation for Python 2
suggests it ought to be possible to use a trampoline to convert Python
2's stackless coroutines into stackful coroutines by manually
implementing a stack.  I also might end up needing a trampoline
anyways to handle the graph-structured stack, in which case the
advantage offered by "yield from" may be diminished.

My tentative analysis is that "yield from" simply isn't useful for
GLL.  GLL's dispatch stack contains information about the GSS and SPPF
as well as which parser needs to resume control, so replacing the
stack with "yield from" isn't going to get me very far since I'll
still need another stack to hold the GSS and SPPF information, even if
it's possible to handle the dispatching without that added
information.  I suspect that it's either impossible to use "yield
from" with GLL or that the added complexity from trying to integrate
the two would completely negate any performance or simplicity gains.
This, combined with the lack of "yield from" in 2.7, means I'm not
going to try to use it.


\subsection{Indices versus Slicing}
\label{sec:indices_slicing}

With Python 2.7+'s memoryviews, it's possible to slice the stream
without copying, embedding slices in the parse forests passed between
combinators.  The main disadvantage of this approach is that it means
the combinators can't handle text, particularly Unicode, since there's
no equivalent for Python strings.  There are also three other factors
to consider: all of the Python library functions (string methods,
struct, re, and so on) take an optional argument that's a starting
index anyways, there's no equivalent stringview object (I'd have to
write one), and indices provide natural labels for the GSS and SPPF
(and memoryviews are only hashable in 3.3+, so they can't take over
that role in 2.7).  Primarily because of the last two considerations,
I'm going with indices.

To my surprise, when I profiled indices versus string slicing
directly, string slicing was slower than indices on both CPython and
PyPy 3, though as expected the indexed version consumed much, much
less memory on CPython.  The extra time seemed to be spent in the
hash-heavy functions, so this might have something to do with
CPython's optimizations for hash tables with string-only keys.  I
still don't understand why the cost of additional allocations didn't
overwhelm that, but this is why we profile.  However, the memory
consumption issue and aforementioned considerations mean I'm sticking
with indices.


\subsection{Module for handling bits}
\label{sec:bits_module}

The main module implemented in C for bit arrays/bit vectors is
\href{https://github.com/ilanschnell/bitarray}{bitarray}
(\href{https://pypi.python.org/pypi/bitarray/}{PyPi}).  I don't know
how hard it will be to make this work with PyPy, but I'm going to give
it a shot.  The main two pure-Python implementations are
\href{https://engineering.purdue.edu/kak/dist/BitVector-3.3.2.html}{BitVector}
(\href{https://pypi.python.org/pypi/BitVector/3.3.2}{PyPi}) and
\href{https://code.google.com/p/python-bitstring/}{bitstring}
(\href{https://pypi.python.org/pypi/bitstring/3.1.3}{PyPi}).  For the
prototype, I'm going with bitstring but will return to bitarray once I
look at performance optimizations.

\subsubsection{bitarray}
\label{sec:bitarray}

This ought to be the fastest because it's implemented in C, but by the
same token, it will probably pose the biggest compatibility problems,
particularly with PyPy.  The API is clean and mimics the standard
sequence types, but has the major disadvantage of having no offset
option in any of the constructors, which would require copying without
using memoryview, and some imperfections in the handling of
conversions (it has a tostring() method, but I don't know if calling
str() works, for instance).  There's no immutable bitarray type, which
would mean that I'd have to make my own.  Bitarray is packaged for
Ubuntu.

\subsubsection{bitstring}
\label{sec:bitstring}

Implemented in pure Python, this ought to be compatible with
everything.  There's also a Cython version, though from looking at it
doesn't seem to have many optimizations over the pure-Python version.
The API includes mutable and immutable types and supports reading from
at a binary object at an arbitrary offset, though obviously it doesn't
support the buffer protocol, and works the way it should with respect
to functions like str().  On the whole, it has clearly the best API.

\subsubsection{BitVector}
\label{sec:bitvector}

Also implemented in pure Python, it ought to have similar
compatibility to bitstring.  The API has the same problem as
bitarray's, no reading binary data at an offset, with the additional
disadvantage of not supporting the buffer protocol.  Moreover, the API
as a whole doesn't resemble that of the standard sequence types.  The
only advantage of this module is that it has built-in methods for
certain kinds of advanced operations on bit arrays, but that's not
that important.


\subsection{Operator Overloading Dispatch for the Combinators}
\label{sec:operator_overloading_dispatch_combinators}

Operator overloading requires type dispatch: even the simplest
possible operator overload has to distinguish between types it accepts
and those it doesn't.  Setting up operator overloading for the
combinators requires more complicated type dispatch.  I wanted to try
to do this with \texttt{functools.singledispatch()} but the
implementation details of Python make this a bad solution.  When
applying the decorator to a function defined in a class body, the
function will always receive as its first argument the instance
calling it.  The author of singledispatch suggests
(\href{http://lukasz.langa.pl/8/single-dispatch-generic-functions/}{What
  single-dispatch generic functions mean for you}) using the decorator
on a function after it's been bound as a method to circumvent this:

\begin{lstlisting}
class C:
    def __init__(self):
        self.method = functools.singledispatch(self.method)
        self.method.register(Type, self.implementation)
\end{lstlisting}

A function wrapping a method only receives the arguments that
\emph{aren't} the instance, so it's possible to dispatch usefully on
its first argument.

\begin{lstlisting}
>>> def f(func):
...   def g(*args):
...     print(args)
...     func()
...   return g
... 
>>> class C:
...   def __init__(self):
...     self.method = f(self.method)
...   def method(self):
...     pass
... 
>>> c = C()
>>> c.method(1, 2, 3)
(1, 2, 3)
\end{lstlisting}

This direct approach doesn't work for operator overloading, however,
because operators are special methods and special methods are looked
up on the class, not the instance: ``For custom classes, implicit
invocations of special methods are only guaranteed to work correctly
if defined on an object's type, not in the object's instance
dictionary''
(\url{https://docs.python.org/3/reference/datamodel.html#special-method-lookup}).

The way to circumvent this is to have the special method delegate to a
normal method and then wrap that method with
\texttt{functools.singledispatch()}:

\begin{lstlisting}
  class AbstractBase:
    def __init__(self) -> None:
        self._op = functools.singledispatch(self._op)
        self._rop = functools.singledispatch(self._rop)

    def __op__(self, other: 'AbstractBase') -> 'AbstractBase':
        return self._op(other)
    def _op(self, other: 'AbstractBase') -> 'AbstractBase':
        return NotImplemented

    def __rop__(self, other: 'AbstractBase') -> 'AbstractBase':
        return self._rop(other)
    def _rop(self, other: 'AbstractBase') -> 'AbstractBase':
        return NotImplemented
\end{lstlisting}

To avoid code duplication for noncommutative operations, however,
there has to be another call to a function/static method that supports
having its arguments reversed.  The following are two classes that
together implement a kind of set of sets monoid with the dispatching
necessary for the operations.  The key observation is that almost all
the work is being done in the staticmethods
\texttt{\_op\_collection\_collection()} and \texttt{\_op\_atom\_atom()}.
The \texttt{\_op\_atom()} and \texttt{\_rop\_atom()} methods on
\texttt{Collection} need to set up calls to \texttt{Collection}
(written as \texttt{type(self)(other)} to avoid hard-coding) and
\texttt{\_op\_collection\_collection()} correctly, and one can argue if
there's content here.  The vast majority of this code is boilerplate,
though.

\begin{lstlisting}
class Collection(AbstractBase):
    def __init__(self, collection: 'Any', *args: 'Atoms') -> None:
        super().__init__()
        self._op.register(Collection, self._op_collection)
        self._op.register(Atom, self._op_atom)
        self._op.register(Collection, self._op_collection)
        self._rop.register(Atom, self._rop_atom)
        self._rop.register(Collection, self._rop_collection)
        self._collection = collection(*args)

    def _op_collection(self, other: 'Collection') -> 'Collection':
        return self._op_collection_collection(self, other)
    def _rop_collection(self, other: 'Collection') -> 'Collection':
        return self._op_collection_collection(other, self)
    def _op_atom(self, other: 'Atom') -> 'Collection':
        return self._op_collection_collection(self, type(self)(other))
    def _rop_atom(self, other: 'Atom') -> 'Collection':
        return self._op_collection_collection(type(self)(other), self)

    @staticmethod
    def _op_collection_collection(left: 'Collection', right:
    'Collection') -> 'Collection':
        return self._collection.op(right._collection)
\end{lstlisting}

\begin{lstlisting}
class Atom(AbstractBase):
    def __init__(self) -> None:
        super().__init__()
        self._op.register(Atom, self._op_atom)
        self._rop.register(Atom, self._rop_atom)

    def _op_atom(self, other: 'Atom') -> 'Collection':
        return self._op_atom_atom(self, other)
    def _rop_atom(self, other: 'Atom') -> 'Collection':
        return self._op_atom_atom(other, self)

    @staticmethod
    def _op_atom_atom(left: 'Atom', right: 'Atom') -> '(Atom, Collection)':
        if oppable(left, right):
            return left.op(right) # -> Atom
        else:
            return Collection(left, right)
\end{lstlisting}

The reason the boilerplate is necessary is that
\texttt{functools.singledispatch()} must dispatch to another method,
since it only received a single argument, with its first argument
being bound implicitly to the instance.  However, the functions that
do the work are generic and need to be functions, not methods, so the
method that is dispatched to needs to explicitly call them with
\texttt{self, other} or \texttt{other, self}.  This is a total of four
function calls, say \texttt{\_\_op\_\_()}, \texttt{\_op()},
\texttt{\_op\_atom()}, and \texttt{\_op\_atom\_atom()}.

Metaprogramming with dynamic class generation can remove the need to
write a lot of the boilerplate and some of the code duplication.
Scary metaprogramming with AST rewriting can, for instance, be used to
turn a method into its reversed form by reversing the variables, which
can eliminate more code duplication and some of the performance
penalty that comes from turning one function call into four function
calls.  However, most of this is a self-created problem relating to
the limitations of \texttt{functools.singledispatch()} for methods,
special methods in particular and using metaprogramming to solve it is
stupid.  Moreover, even with metaprogramming, there's a nontrivial
performance hit.  Worse, it will make the implementation complex and
hard to understand and maintain, and I don't know if it's even
possible to keep the metaprogramming completely closed off from the
rest of the API.  Another, much less significant problem with
\texttt{functools.singledispatch()} is that it doesn't integrate with
function annotations like it should (probably for backwards
compatibility), which means refactoring to come if I use it.

There's nothing wrong with the theory of emulating double dispatch
with two levels of single dispatch, it just doesn't work with
\texttt{functools.singledispatch()'s} implementation.  There's a long
list of multiple-dispatch implementations for Python (including
\url{https://pypi.python.org/pypi/multimethods},
\url{https://pypi.python.org/pypi/generic},
\url{https://pypi.python.org/pypi/multipledispatch/},
\url{https://github.com/morepath/reg}, an @overload decorator in
\href{http://mypy-lang.org/}{Mypy} that was removed during the
discussions about type-hinting for Python 3.5, and others that are
older) that could handle the necessary dispatch, but they're all
varying degrees of unsafe (most of them use frame inspection) and
overcomplicated.  The latter is a problem because multiple dispatch is
overkill for this problem, since the type of \texttt{self} for any
given special method is known at compile time.  Moreover, all the
implementations differ in both syntax and important details, so
there's not anything close to a standard.  Guido van Rossum claims to
be looking at multiple dispatch again for the future, but who knows if
that's going to come to pass.  The lack of standardization and
possible upcoming changes to singledispatch and the typing system in
general and the possibility of real multiple dispatch in the standard
library make any solution not very future-proof.

The central problem with dispatch for special methods is that at class
creation, a special method doesn't know what instances will be
created, but it has to be capable of dispatching to instance methods
to use object-oriented method dispatch.  Thus, the special method can
only dispatch to an instance method when it's called, when it has
access to the instance via its first argument.  By having the special
method itself do the dispatching, rather than a wrapper function, and
dispatching on the type of the \emph{second} argument, I can avoid
these problems and use a form of chained single-dispatch to do the
necessary double dispatch.  The best solution would probably be to use
the same dispatching for subtypes that
\texttt{functools.singledispatch()} uses.  However, the code for
\texttt{functools.singledispatch()} isn't intended to be used outside
its module, with two functions created inside the
\texttt{functools.singledispatch()} function itself.  Thus, I instead
implemented my own dispatching decorator in the simplest, dumbest
possible way with no dispatch to supertypes at all.


\section{Notes}
\label{sec:notes}


\subsection{Lookahead Algorithms for GLL}
\label{sec:gll_lookahead}

One possibility for increasing the set of languages that GLL can parse
in linear time is to use a different lookahead algorithm.  ANTLR3 and
ANTLR4 more or less directly use DFAs to predict which alternative to
pick and thus enable a conventional top-down parser to work on a set
the authors call LL(*) \parencites{antlr3, antlr4}, which I think is
equivalent to LL-regular (though LL-regular is a proper subset of the
LR-regular languages\parencite{ll-regular}), though with a worst-case
time complexity of $O(n^4)$.  It's possible the same approach might
work for GLL to allow it to run in sub-quadratic time on LL-regular
grammars or to improve its constant factors by allowing it to run in
recursive-descent mode on a broader class of inputs.  To improve its
worst-case asymptotic performance, the lookahead would have to
terminate deterministically before reading the whole input.


\subsection{Afroozeh/Ismaylova's versus Spiewak's GSS}
\label{sec:gss_details}


\subsection{Yakker SPPF}
\label{sec:yakker_sppf}


\subsection{Type Checking}
\label{sec:type_checking}

Ideally combinators should fail as soon as someone attempts to combine
combinators that process data of different types (Unicode/text and
binary data, at the moment) or parse data of an inappropriate type for
a given combinator, rather than failing deep into parsing.
Unfortunately, two things make this difficult: mutually recursive
functions create cycles in the digraph through which types propagate,
and lazy evaluation of combinators makes it hard to analyze types at
class instantiation time.  A correct solution to this problem is
almost certainly going to involve using one of the existing
type-checking/type-inference algorithms, and it might make sense to
outsource it to something like mypy.  Arguably, it isn't in the spirit
of Python's dynamic type system since it involves a form of static
analysis, though the addition of gradual typing to 3.5 undermines this
argument.

Since combinators pass the same input stream among themselves, to
avoid generating runtime errors due to passing text to terminal
combinators expecting binary data or binary data to combinators
expecting text, all combinators will be assigned as either text or
binary data.  Higher-order combinators inherit their type from their
sub-combinators while terminals have their data type explicitly
defined.  While some terminals are always binary (struct), others
depend on how they're initialized (re, string.startswith, and possibly
bits depending on which backend I use and how I set up the API).  In
Python 2, objects of type 'str' are treated as binary data and objects
of type 'unicode' are treated as text.  In Python 3, objects of type
'str' are treated as text and types 'bytes' and 'bytearray' are
treated as binary data.  Objects of type 'memoryview' are binary on
both.  This can lead to weird situations where the Strings combinator
accepts binary data, but that's a consequence of Python's design and
not something I can do anything about.

Testing on 3.4 and 2.7 suggests that struct doesn't actually care what
it's given, bytes, str, or unicode, but enforcing consistency on its
input won't hurt.  array accepts *only* native strings, so str/bytes
on Python 2 and str/unicode on Python 3.


\subsection{Parse-forest grammars and adjacency-list digraphs}
\label{sec:parse-forest_grammars_adjacency-list_digraphs}

My original implementation of trees/forests as digraphs using
adjacency lists containing node labels (rather than node references)
is very close (I haven't done the detailed analysis to determine how
close) to a parse-forest grammar.  I don't know if there are uses for
this interpretation, but it's something to keep in mind especially
when considering code generation and error handling.


\subsection{Arrow combinators}
\label{sec:arrow_combinators}

Hughes notes that any arrow that supports \emph{apply} is, in effect,
a monad.  He makes the further observation that \emph{apply} and the
choice operator can't be defined for the kind of parser that Swiestra
and Duponcheel discuss and states, ``Luckily, this does not matter: it
is rare that we \emph{want} to write a parser which decides on the
grammar to accept on the basis of previously parsed values.''  Of
course, since this is exactly what I'm trying to do, using arrows here
would be counterproductive.  What this means exactly for integrating
any of Swierstra and Duponcheel's ideas into my combinators I'm not
sure: is there some kind of model that can incorporate both the
necessary dependence on past parse results and the separation of
static and dynamic elements in arrows?  I don't know.


\subsection{Tunnel / Monadic Bind?}
\label{sec:tunnel_bind}

Is there a connection?  I actually think I can implement Tunnel with
Act/>>.



\printbibliography

\end{document}
